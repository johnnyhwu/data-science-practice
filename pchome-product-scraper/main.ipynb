{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CollectAllProductURL Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CollectAllProductURL(searchpage_url, total_page):\n",
    "    pchome_url = \"https://www.pcstore.com.tw\"\n",
    "    result = []\n",
    "    num_page = 1\n",
    "    \n",
    "    while num_page <= total_page:\n",
    "        time.sleep(random.randint(3,5))\n",
    "        \n",
    "        html = requests.get(searchpage_url)\n",
    "        html.encoding = \"big5\"\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        \n",
    "        for td in soup.find_all(\"td\", class_=\"list_proName\"):\n",
    "            result.append(pchome_url+td.find(\"a\")[\"href\"])\n",
    "            \n",
    "        num_page += 1\n",
    "        searchpage_url = searchpage_url.replace(\"page={}\".format(str(num_page-1)), \"page={}\".format(str(num_page)))        \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SrapeData Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapData(url_lst):\n",
    "    all_dicts = []\n",
    "    \n",
    "    for url in url_lst:\n",
    "        product_dict = {}\n",
    "        \n",
    "        time.sleep(random.randint(3,15))\n",
    "        html = requests.get(url)\n",
    "        html.encoding = 'big5'\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        \n",
    "        # find product title\n",
    "        try:\n",
    "            name = soup.find(\"h1\")\n",
    "            name = name.text\n",
    "            product_dict[\"name\"] = name\n",
    "        except:\n",
    "            print(\"Exception: {} fails to extract name.\".format(url))\n",
    "            continue\n",
    "            \n",
    "        # find product price\n",
    "        try:\n",
    "            dollar = soup.find_all(\"td\", class_=\"t13\")\n",
    "            \n",
    "            original_price = dollar[0].find(\"del\")\n",
    "            original_price = original_price.text\n",
    "            product_dict[\"original_price\"] = original_price\n",
    "            \n",
    "            special_price = dollar[1].find(\"span\")\n",
    "            special_price = special_price.text\n",
    "            product_dict[\"special_price\"] = special_price\n",
    "        except:\n",
    "            print(\"Exception: {} fails to extract price.\".format(url))\n",
    "            continue\n",
    "            \n",
    "        # find imgs & contents: find table\n",
    "        objective_idx = -1\n",
    "        for idx,table in enumerate(soup.find_all(\"table\")):\n",
    "            table = str(table)\n",
    "            if \"技術規格\" in table:\n",
    "                objective_idx = idx\n",
    "                break\n",
    "        \n",
    "        if objective_idx == -1:\n",
    "            print(\"Exception: {} fails to find idx.\".format(url))\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        # find imgs & contents: extract table\n",
    "        try:\n",
    "            tr = soup.find_all(\"table\")[22].find_all(\"tr\")[3]\n",
    "        except:\n",
    "            print(\"Exception: {} fails to extract table.\".format(url))\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # find imgs\n",
    "        try:\n",
    "            num_img = len(tr.find_all(\"img\"))\n",
    "            if num_img > 1:\n",
    "                print(\"Exception: {} images > 1.\".format(url))\n",
    "                continue\n",
    "                \n",
    "            img_url = tr.find(\"img\")[\"src\"]\n",
    "            product_dict[\"image\"] = img_url\n",
    "        except:\n",
    "            print(\"Exception: {} fails to extract image.\".format(url))\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        # find content 1\n",
    "        contents = \"\"\n",
    "        \n",
    "        try:\n",
    "            content1 = tr.find_all(\"font\")\n",
    "            for string in content1:\n",
    "                contents += string.text\n",
    "                contents += \"\\n\"\n",
    "        except:\n",
    "            print(\"Exception: {} fails to extract content1.\".format(url))\n",
    "            continue\n",
    "        \n",
    "        # find content 2\n",
    "        try:\n",
    "            content2 = tr.find_all(\"span\")\n",
    "            for string in content2:\n",
    "                contents += string.text\n",
    "                contents += \"\\n\"\n",
    "        except:\n",
    "            print(\"Exception: {} fails to extract content2.\".format(url))\n",
    "            continue\n",
    "            \n",
    "        product_dict[\"contents\"] = contents\n",
    "            \n",
    "        all_dicts.append(product_dict)\n",
    "        qbar.update(1)\n",
    "    \n",
    "    return all_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchpage_url = \"https://www.pcstore.com.tw/obravo/HM/search.htm?st_sort=2&s_page=1&pql_word=IhbfGu6..GHHNoeq-TIge8gse2Iw25OLxGCDaogLjHAe3Oeig2UjUzXse2Iw25OLxG3kk\"\n",
    "all_products_url = CollectAllProductURL(searchpage_url, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6514ac57378f49828e13bf843e843faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: https://www.pcstore.com.tw/obravo/M55108756.htm fails to extract table.\n",
      "Exception: https://www.pcstore.com.tw/obravo/M55018834.htm images > 1.\n",
      "Exception: https://www.pcstore.com.tw/obravo/M55040523.htm images > 1.\n",
      "Exception: https://www.pcstore.com.tw/obravo/M08547616.htm fails to find idx.\n",
      "Exception: https://www.pcstore.com.tw/obravo/M08547417.htm fails to find idx.\n",
      "Exception: https://www.pcstore.com.tw/obravo/M24223886.htm fails to find idx.\n",
      "Exception: https://www.pcstore.com.tw/obravo/M24224968.htm images > 1.\n"
     ]
    }
   ],
   "source": [
    "qbar = tqdm(total=len(all_products_url))\n",
    "all_dicts = ScrapData(all_products_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StoreData.pickle\",\"wb\") as file:\n",
    "    pickle.dump(all_dicts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_dicts)\n",
    "df = df.reindex([\"name\", \"original_price\", \"special_price\", \"image\", \"contents\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"商品名稱\", \"原價\", \"特價\", \"圖片\", \"描述\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"whai.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
