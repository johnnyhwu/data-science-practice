{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Import Package"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport glob\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Show Training Image"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data_path = \"../input/horses-or-humans-dataset/horse-or-human/train/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"horse_image_name = glob.glob(training_data_path + \"horses/*png\")\nhuman_image_name = glob.glob(training_data_path + \"humans/*png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of horse images: \", len(horse_image_name))\nprint(\"Number of human images: \", len(human_image_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_image_label(images, label, is_path=True):\n    \n    idx = 0\n    fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\n    \n    for i in range(5):\n        for j in range(5):\n            \n            if is_path:\n                img = cv2.imread(images[idx])\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            else:\n                img = images[idx]\n                \n            idx += 1\n            \n            axes[i, j].set_title(label=label, color=\"green\", fontsize=15)\n            axes[i, j].set_xticks([])\n            axes[i, j].set_yticks([])\n            axes[i, j].imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image_label(horse_image_name[0:25], \"horse\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image_label(human_image_name[0:25], \"human\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create ImageDataGenerator"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rotation_range=270,\n                                   width_shift_range=0.1,\n                                   # height_shift_range=0.5,\n                                   zoom_range=0.15,\n                                   # horizontal_flip=True,\n                                   # vertical_flip=True,\n                                   rescale=1/255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_generator = train_datagen.flow_from_directory(directory=training_data_path,\n                                  target_size=(300, 300),\n                                  batch_size=25,\n                                  class_mode=\"categorical\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_data = training_generator.next()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_generator.batch_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image_label(batch_data[0], \"Not Sure\", is_path=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_generator.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_generator.batch_index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Model"},{"metadata":{},"cell_type":"markdown","source":"### Architechture"},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = keras.layers.Input(shape=(300, 300, 3))\nx = keras.layers.Conv2D(filters=4, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(inputs)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(units=128, activation=tf.nn.relu)(x)\nx = keras.layers.Dense(units=16, activation=tf.nn.relu)(x)\noutputs = keras.layers.Dense(units=2, activation=tf.nn.softmax)(x)\n\nmodel = keras.models.Model(inputs=inputs, outputs=outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compilation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"acc\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Callback"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomeCallback(keras.callbacks.Callback):\n    \n    def on_epoch_end(self, epoch, logs):\n        \n        if logs[\"acc\"] >= 0.97:\n            print(\"Model's accuracy is enough !\")\n            self.model.stop_training = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_callback = CustomeCallback()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=training_generator, epochs=50, verbose=1, callbacks=[custom_callback], steps_per_epoch=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"label = [\"horse\", \"human\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(path):\n    \n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (300, 300))\n    \n    img = np.reshape(img, (1, 300, 300, 3))\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = load_image(\"../input/horse-breeds/01_005.png\")\nlabel[np.argmax(model.predict(img))]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}